# Arc Configuration File
# ======================
# Production-ready defaults for Arc time-series data platform
#
# This file uses TOML configuration format.
# Environment variables override these settings.
# Example: ARC_WORKERS=8 will override [server].workers setting

# ======================
# Server Configuration
# ======================
[server]
# Host to bind API server (default: 0.0.0.0 for all interfaces)
host = "0.0.0.0"

# Port for API server (default: 8000)
port = 8000

# Number of worker processes for high-throughput ingestion
# Recommended values:
#   1  = Single worker (~10K rps, 300-400 MB RAM)
#   4  = Light load (~40K rps, ~1.6 GB RAM)
#   8  = Medium load (~80K rps, ~3.2 GB RAM)
#   16 = High load (~160K rps, ~6.4 GB RAM)
#   0  = Auto-detect (uses CPU count)
# Default: 4 (Gunicorn multi-worker mode)
workers = 8

# Worker timeout in seconds (default: 120)
worker_timeout = 120

# Graceful shutdown timeout (default: 60)
graceful_timeout = 60

# Max requests per worker before restart (prevents memory leaks)
max_requests = 50000
max_requests_jitter = 5000

# Worker connections (concurrent requests per worker)
worker_connections = 1000

# ======================
# Authentication
# ======================
[auth]
# Enable API token authentication (recommended: true)
enabled = true

# Default API token (leave empty to auto-generate on first run)
# IMPORTANT: Change this in production!
default_token = ""

# Endpoints that don't require authentication (comma-separated)
allowlist = "/health,/ready,/docs,/openapi.json,/auth/verify"

# ======================
# Query Cache
# ======================
[query_cache]
# Enable query result caching (recommended: true for dashboards)
enabled = true

# Cache time-to-live in seconds (default: 60)
# Recommended:
#   30-60s   = Dashboards with frequent updates
#   120-300s = Dashboards with less frequent updates
ttl_seconds = 60

# Maximum number of cached queries (default: 100)
# Memory usage: ~1-5 MB per cached query
max_size = 100

# Maximum result size to cache in MB (default: 10)
# Larger results won't be cached to save memory
max_result_mb = 10

# ======================
# DuckDB Query Engine
# ======================
[duckdb]
# Connection pool size per worker (default: 5)
# Increase for high concurrent query workloads
pool_size = 5

# Maximum queries to queue when pool is full (default: 100)
# Recommended: 20-30x pool size
max_queue_size = 100

# Enable Parquet metadata caching (default: true)
# Caches file metadata for faster repeated queries
# Provides 2-10x speedup for dashboard workloads
enable_object_cache = true

# ======================
# Data Ingestion (Line Protocol)
# ======================
[ingestion]
# Write buffer size (records before flush)
# Recommended:
#   50,000   = Low memory, many small files (needs frequent compaction)
#   200,000  = Balanced (recommended for most workloads) ⭐
#   500,000  = High throughput, fewer files, more memory
# Impact: Larger buffers = fewer files = faster compaction & queries
buffer_size = 200000

# Max buffer age in seconds before flush
# Recommended:
#   5-10s  = Real-time dashboards ⭐
#   15-30s = Batch ingestion
# Impact: Longer age = fewer files but older data in queries
buffer_age_seconds = 10

# Compression for parquet files (snappy, gzip, zstd, none)
# snappy = Fast compression (recommended for ingestion)
# zstd   = Better compression (used by compaction automatically)
compression = "snappy"

# ======================
# Write-Ahead Log (WAL) - Durability Feature
# ======================
# WAL provides zero data loss guarantees on system crashes.
# DISABLED BY DEFAULT - Enable only if you need guaranteed durability.
#
# Performance Impact:
#   Disabled:         2.01M records/sec (0-5 seconds data loss risk on crash)
#   WAL + fdatasync:  1.63M records/sec (-19%, near-zero data loss)
#   WAL + fsync:      1.67M records/sec (-17%, zero data loss)
#   WAL + async:      1.67M records/sec (-17%, <1s data loss risk)
#
# When to enable WAL:
#   ✓ Financial/regulated industries requiring zero data loss
#   ✓ Critical data that cannot be lost
#   ✓ No upstream retry mechanism (Kafka, message queues)
#
# When to keep WAL disabled:
#   ✓ Maximum throughput is priority
#   ✓ Can tolerate 0-5 seconds data loss on rare crashes
#   ✓ Have upstream retry logic
#
[wal]
# Enable Write-Ahead Log (default: false)
enabled = false

# WAL directory (each worker creates separate WAL file)
# Use fast storage (NVMe SSD) for best performance
dir = "./data/wal"

# Sync mode: fsync, fdatasync, async
#   fsync:     Slowest, maximum safety (data + metadata synced)
#   fdatasync: Recommended, balanced (data synced, metadata async)
#   async:     Fastest, relies on OS buffer cache (small risk)
sync_mode = "fsync"

# Rotate WAL file when it reaches this size (MB)
# Smaller files = faster recovery, more files to manage
# Larger files = slower recovery, fewer files
max_size_mb = 100

# Rotate WAL file after this many seconds
# Recommended: 3600 (1 hour)
max_age_seconds = 3600

# Example configurations for different use cases:
#
# High Durability (Financial/Healthcare):
#   enabled = true
#   sync_mode = "fsync"
#   max_size_mb = 50
#   max_age_seconds = 1800
#
# Balanced (Production):
#   enabled = true
#   sync_mode = "fdatasync"
#   max_size_mb = 100
#   max_age_seconds = 3600
#
# Performance-First (Can tolerate small loss):
#   enabled = true
#   sync_mode = "async"
#   max_size_mb = 200
#   max_age_seconds = 7200
#
# Maximum Throughput (No WAL):
#   enabled = false

# ======================
# Storage Backend
# ======================
[storage]
# Backend type: local, minio, s3, gcs, ceph
#
# local  = Direct filesystem (fastest, for single-node deployments)
# minio  = MinIO object storage (for distributed/development)
# s3     = AWS S3, Cloudflare R2, or S3-compatible storage
# gcs    = Google Cloud Storage
# ceph   = Ceph object storage
backend = "local"

# Local Filesystem Configuration (if backend = local)
# Best for: Single-node deployments, development, maximum performance
[storage.local]
# Base directory for data storage
# Use absolute path for production, relative path for development
# Examples:
#   base_path = "./data/arc"           # Relative to Arc installation
#   base_path = "/data/arc"            # Absolute path
#   base_path = "/mnt/nvme/arc-data"   # Fast NVMe storage
base_path = "./data/arc"

# Database namespace (default, production, staging, etc.)
database = "default"

# MinIO Configuration (if backend = minio)
[storage.minio]
endpoint = "http://localhost:9000"
access_key = "minioadmin"
secret_key = "minioadmin"
bucket = "arc"
database = "default"  # Database namespace (default, production, staging, etc.)
use_ssl = false

# AWS S3 Configuration (if backend = s3)
[storage.s3]
# access_key = ""
# secret_key = ""
# bucket = "arc-data"
# region = "us-east-1"
# database = "default"  # Database namespace

# GCS Configuration (if backend = gcs)
[storage.gcs]
# bucket = "arc-data"
# project_id = ""
# credentials_file = "/path/to/credentials.json"
# database = "default"  # Database namespace

# ======================
# Logging
# ======================
[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# Log format: structured (JSON) or text
format = "structured"

# Include stack traces in logs
include_trace = false

# ======================
# CORS (Cross-Origin Resource Sharing)
# ======================
[cors]
# Comma-separated allowed origins
# Use "*" for allow all (not recommended in production)
origins = "http://localhost:3000,http://atila:3000"

# ======================
# Compaction - File Optimization
# ======================
# Compaction merges small Parquet files into larger files for better query performance.
# ENABLED BY DEFAULT - Compaction is essential for production deployments.
#
# Why compaction matters:
#   - Arc creates many small files during high-throughput ingestion
#   - Small files = slow queries (DuckDB must open hundreds of files)
#   - Compaction reduces file count by 100x, improves query speed by 10-50x
#
[compaction]
# Enable automatic compaction (recommended: true)
enabled = true

# Compact partitions older than this (don't compact active hour)
# Recommended: 1 hour (wait for hour to complete before compacting)
min_age_hours = 0

# Only compact partitions with at least this many files
# Lower = more aggressive compaction, higher = less overhead
min_files = 10

# Target file size for compacted Parquet files (MB)
# Recommended: 512MB (DuckDB's sweet spot for parallel scanning)
#   256MB = More granular, faster compaction
#   512MB = Balanced (recommended)
#   1024MB = Fewer files, best compression
target_file_size_mb = 512

# Compaction schedule (cron syntax: minute hour day month weekday)
# Default: "5 * * * *" = Every hour at :05 (e.g., 1:05am, 2:05am)
# This gives the hour 5 minutes to "settle" before compaction
#
# Examples:
#   "5 * * * *"        - Every hour at :05 (recommended)
#   "0 2,14 * * *"     - 2am and 2pm daily
#   "*/15 * * * *"     - Every 15 minutes (aggressive)
#   "0 3 * * 0"        - 3am every Sunday (weekly)
schedule = "5 * * * *"

# Max concurrent compaction jobs
# Recommended: 2-4 (balance between speed and resource usage)
max_concurrent_jobs = 2

# Lock time-to-live in hours (for crash recovery)
# If compaction crashes, lock expires after this time
lock_ttl_hours = 2

# Temp directory for compaction work
# Should have enough space for largest hourly partition
temp_dir = "./data/compaction"

# Performance tuning
compression = "zstd"           # zstd = better compression, snappy = faster
compression_level = 3          # 1-9 (higher = better compression, slower)

# ======================
# Monitoring & Metrics
# ======================
[monitoring]
# Enable Prometheus metrics endpoint at /metrics
# enabled = true

# Metrics collection interval in seconds
# collection_interval = 30

# ======================
# Data Source (Optional - can be configured via UI)
# ======================
[datasource]
# Type: influx, timescale, http_json
# type = "influx"

# InfluxDB 1.x Configuration
# [datasource.influx]
# host = "influxdb"
# port = 8086
# database = "historian_test"
# username = "historian"
# password = "historian123"

# ======================
# Advanced Settings
# ======================
[advanced]
# Enable experimental features
# experimental = false

# Arrow Flight server (high-performance streaming)
# arrow_flight_enabled = false
# arrow_flight_port = 8081
