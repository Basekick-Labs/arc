# Arc Configuration File
# ======================
# Production-ready defaults for Arc time-series data platform
#
# This file uses TOML configuration format.
# Environment variables override these settings.
# Example: ARC_WORKERS=8 will override [server].workers setting

# ======================
# Server Configuration
# ======================
[server]
# Host to bind API server (default: 0.0.0.0 for all interfaces)
host = "0.0.0.0"

# Port for API server (default: 8000)
port = 8000

# Number of worker processes for high-throughput ingestion
# Recommended values:
#   1  = Single worker (~10K rps, 300-400 MB RAM)
#   4  = Light load (~40K rps, ~1.6 GB RAM)
#   8  = Medium load (~80K rps, ~3.2 GB RAM)
#   16 = High load (~160K rps, ~6.4 GB RAM)
#   0  = Auto-detect (uses CPU count)
# Default: 4 (Gunicorn multi-worker mode)
workers = 8

# Worker timeout in seconds (default: 120)
worker_timeout = 120

# Graceful shutdown timeout (default: 60)
graceful_timeout = 60

# Max requests per worker before restart (prevents memory leaks)
max_requests = 50000
max_requests_jitter = 5000

# Worker connections (concurrent requests per worker)
worker_connections = 1000

# ======================
# Authentication
# ======================
[auth]
# Enable API token authentication (recommended: true)
enabled = true

# Default API token (leave empty to auto-generate on first run)
# IMPORTANT: Change this in production!
default_token = ""

# Endpoints that don't require authentication (comma-separated)
allowlist = "/health,/ready,/docs,/openapi.json,/auth/verify"

# ======================
# Query Cache
# ======================
[query_cache]
# Enable query result caching (recommended: true for dashboards)
enabled = true

# Cache time-to-live in seconds (default: 60)
# Recommended:
#   30-60s   = Dashboards with frequent updates
#   120-300s = Dashboards with less frequent updates
ttl_seconds = 60

# Maximum number of cached queries (default: 100)
# Memory usage: ~1-5 MB per cached query
max_size = 100

# Maximum result size to cache in MB (default: 10)
# Larger results won't be cached to save memory
max_result_mb = 10

# ======================
# DuckDB Query Engine
# ======================
[duckdb]
# Connection pool size per worker (default: 5)
# Increase for high concurrent query workloads
# Recommended:
#   5  = Low concurrency (1-10 concurrent queries)
#   10 = Medium concurrency (10-50 concurrent queries)
#   20 = High concurrency (50+ concurrent queries)
pool_size = 5

# Maximum queries to queue when pool is full (default: 100)
# Recommended: 20-30x pool size
max_queue_size = 100

# Production Performance Tuning
# ==============================
# Memory limit per DuckDB connection (e.g., "4GB", "8GB", "16GB")
# Recommended: Total RAM / (workers * pool_size) with 20% headroom
# Example: 64GB RAM, 8 workers, 5 pool_size = 64 / (8*5) * 0.8 = ~1.6GB per connection
# Set higher for complex analytical queries, lower to prevent OOM
memory_limit = "8GB"

# Number of threads per DuckDB connection (default: CPU cores)
# Recommended: Match CPU cores for single-query workloads, lower for high concurrency
# - Single user/dashboard: CPU cores (e.g., 14 for M3 Max)
# - Multi-user: CPU cores / pool_size (e.g., 14/5 = 2-3 threads)
# - High concurrency: 1-2 threads (let pool handle parallelism)
threads = 14

# Temporary directory for DuckDB spill-to-disk operations
# Used when queries exceed memory_limit
# Should be on fast storage (NVMe SSD recommended)
temp_directory = "./data/duckdb_tmp"

# Enable Parquet metadata caching (recommended: true)
# Caches file metadata for faster repeated queries on same files
# MAJOR PERFORMANCE BOOST: 2-10x faster for queries on same files
enable_object_cache = true

# ======================
# Delete Operations
# ======================
[delete]
# Enable delete operations (default: false for safety)
# Must be explicitly enabled to allow data deletion
enabled = true

# Require explicit confirmation for large deletes (default: 10000 rows)
# Deletes affecting more than this threshold require confirm=true parameter
confirmation_threshold = 10000

# Maximum rows that can be deleted in a single operation (default: 1000000)
# Prevents accidental massive deletions
max_rows_per_delete = 1000000

# Tombstone retention period in days (default: 30)
# Deleted data markers kept for this period before physical removal during compaction
# Set to 0 to remove immediately on next compaction
tombstone_retention_days = 0

# Enable audit logging of delete operations (default: true)
# Logs all delete operations with timestamp, user, and WHERE clause
audit_enabled = true

# ======================
# Data Ingestion (Line Protocol)
# ======================
[ingestion]
# Write buffer size (records before flush)
# Recommended:
#   50,000   = Low memory, many small files (needs frequent compaction)
#   200,000  = Balanced (recommended for most workloads) ⭐
#   500,000  = High throughput, fewer files, more memory
# Impact: Larger buffers = fewer files = faster compaction & queries
buffer_size = 17000

# Max buffer age in seconds before flush
# Recommended:
#   5-10s  = Real-time dashboards ⭐
#   15-30s = Batch ingestion
# Impact: Longer age = fewer files but older data in queries
buffer_age_seconds = 5

# Compression for parquet files (snappy, gzip, zstd, none)
# snappy = Fast compression (recommended for ingestion)
# zstd   = Better compression (used by compaction automatically)
compression = "snappy"

# ======================
# Write-Ahead Log (WAL) - Durability Feature
# ======================
# WAL provides zero data loss guarantees on system crashes.
# DISABLED BY DEFAULT - Enable only if you need guaranteed durability.
#
# Performance Impact:
#   Disabled:         2.01M records/sec (0-5 seconds data loss risk on crash)
#   WAL + fdatasync:  1.63M records/sec (-19%, near-zero data loss)
#   WAL + fsync:      1.67M records/sec (-17%, zero data loss)
#   WAL + async:      1.67M records/sec (-17%, <1s data loss risk)
#
# When to enable WAL:
#   ✓ Financial/regulated industries requiring zero data loss
#   ✓ Critical data that cannot be lost
#   ✓ No upstream retry mechanism (Kafka, message queues)
#
# When to keep WAL disabled:
#   ✓ Maximum throughput is priority
#   ✓ Can tolerate 0-5 seconds data loss on rare crashes
#   ✓ Have upstream retry logic
#
[wal]
# Enable Write-Ahead Log (default: false)
enabled = false

# WAL directory (each worker creates separate WAL file)
# Use fast storage (NVMe SSD) for best performance
dir = "./data/wal"

# Sync mode: fsync, fdatasync, async
#   fsync:     Slowest, maximum safety (data + metadata synced)
#   fdatasync: Recommended, balanced (data synced, metadata async)
#   async:     Fastest, relies on OS buffer cache (small risk)
sync_mode = "fsync"

# Rotate WAL file when it reaches this size (MB)
# Smaller files = faster recovery, more files to manage
# Larger files = slower recovery, fewer files
max_size_mb = 100

# Rotate WAL file after this many seconds
# Recommended: 3600 (1 hour)
max_age_seconds = 3600

# Example configurations for different use cases:
#
# High Durability (Financial/Healthcare):
#   enabled = true
#   sync_mode = "fsync"
#   max_size_mb = 50
#   max_age_seconds = 1800
#
# Balanced (Production):
#   enabled = true
#   sync_mode = "fdatasync"
#   max_size_mb = 100
#   max_age_seconds = 3600
#
# Performance-First (Can tolerate small loss):
#   enabled = true
#   sync_mode = "async"
#   max_size_mb = 200
#   max_age_seconds = 7200
#
# Maximum Throughput (No WAL):
#   enabled = false

# ======================
# Storage Backend
# ======================
[storage]
# Backend type: local, minio, s3, gcs, ceph
#
# local  = Direct filesystem (fastest, for single-node deployments)
# minio  = MinIO object storage (for distributed/development)
# s3     = AWS S3, Cloudflare R2, or S3-compatible storage
# gcs    = Google Cloud Storage
# ceph   = Ceph object storage
backend = "local"

# Local Filesystem Configuration (if backend = local)
# Best for: Single-node deployments, development, maximum performance
[storage.local]
# Base directory for data storage
# Use absolute path for production, relative path for development
# Examples:
#   base_path = "./data/arc"           # Relative to Arc installation
#   base_path = "/data/arc"            # Absolute path
#   base_path = "/mnt/nvme/arc-data"   # Fast NVMe storage
base_path = "./data/arc"

# Database namespace (default, production, staging, etc.)
database = "default"

# MinIO Configuration (if backend = minio)
[storage.minio]
endpoint = "http://localhost:9000"
access_key = "minioadmin"
secret_key = "minioadmin"
bucket = "arc"
database = "default"  # Database namespace (default, production, staging, etc.)
use_ssl = false

# AWS S3 Configuration (if backend = s3)
[storage.s3]
# access_key = ""
# secret_key = ""
# bucket = "arc-data"
# region = "us-east-1"
# database = "default"  # Database namespace

# GCS Configuration (if backend = gcs)
[storage.gcs]
# bucket = "arc-data"
# project_id = ""
# credentials_file = "/path/to/credentials.json"
# database = "default"  # Database namespace

# ======================
# Logging
# ======================
[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# Log format: structured (JSON) or text
format = "structured"

# Include stack traces in logs
include_trace = false

# ======================
# CORS (Cross-Origin Resource Sharing)
# ======================
[cors]
# Comma-separated allowed origins
# Use "*" for allow all (not recommended in production)
origins = "http://localhost:3000,http://atila:3000"

# ======================
# Compaction - File Optimization
# ======================
# Compaction merges small Parquet files into larger files for better query performance.
# ENABLED BY DEFAULT - Compaction is essential for production deployments.
#
# Why compaction matters:
#   - Arc creates many small files during high-throughput ingestion
#   - Small files = slow queries (DuckDB must open hundreds of files)
#   - Compaction reduces file count by 100x, improves query speed by 10-50x
#
[compaction]
# Enable automatic compaction (recommended: true)
enabled = true

# Compact partitions older than this (don't compact active hour)
# Recommended: 1 hour (wait for hour to complete before compacting)
min_age_hours = 0

# Only compact partitions with at least this many files
# Lower = more aggressive compaction, higher = less overhead
min_files = 10

# Target file size for compacted Parquet files (MB)
# Recommended: 512MB (DuckDB's sweet spot for parallel scanning)
#   256MB = More granular, faster compaction
#   512MB = Balanced (recommended)
#   1024MB = Fewer files, best compression
target_file_size_mb = 512

# Compaction schedule (cron syntax: minute hour day month weekday)
# Default: "5 * * * *" = Every hour at :05 (e.g., 1:05am, 2:05am)
# This gives the hour 5 minutes to "settle" before compaction
#
# Examples:
#   "5 * * * *"        - Every hour at :05 (recommended)
#   "0 2,14 * * *"     - 2am and 2pm daily
#   "*/15 * * * *"     - Every 15 minutes (aggressive)
#   "0 3 * * 0"        - 3am every Sunday (weekly)
schedule = "5 * * * *"

# Max concurrent compaction jobs
# Recommended: 2-4 (balance between speed and resource usage)
max_concurrent_jobs = 2

# Lock time-to-live in hours (for crash recovery)
# If compaction crashes, lock expires after this time
lock_ttl_hours = 2

# Temp directory for compaction work
# Should have enough space for largest hourly partition
temp_dir = "./data/compaction"

# Performance tuning
compression = "zstd"           # zstd = better compression, snappy = faster
compression_level = 3          # 1-9 (higher = better compression, slower)

# ======================
# Continuous Queries - Downsampling & Aggregation
# ======================
# Continuous queries allow automatic downsampling and aggregation of time-series data.
# MANUAL EXECUTION ONLY - Automatic scheduling is reserved for enterprise edition.
#
# Use cases:
#   - Downsample high-resolution data (e.g., 10s → 1m → 1h → 1d)
#   - Pre-compute aggregated metrics for dashboards
#   - Create summary tables for long-term analysis
#
[continuous_queries]
# Enable continuous queries (default: true)
enabled = true

# Maximum concurrent query executions
max_concurrent = 2

# Query timeout in seconds (default: 3600 = 1 hour)
timeout = 3600

# Temp directory for staging aggregated data before writing
temp_dir = "./data/cq_temp"

# ======================
# Telemetry
# ======================
[telemetry]
# Enable anonymous usage telemetry (default: true)
# Telemetry helps us understand usage patterns and improve Arc.
# All data collected is transparent and sent in readable JSON format.
#
# Data collected:
# - Instance ID (random UUID, not tied to hardware)
# - OS name, version, and architecture
# - CPU core count
# - Total RAM
# - Arc version
#
# To disable telemetry, set this to false:
enabled = true

# Telemetry server endpoint
endpoint = "https://telemetry.basekick.net/api/v1/telemetry"

# Hours between telemetry reports (default: 24)
interval_hours = 24

# ======================
# Monitoring & Metrics
# ======================
[monitoring]
# Enable Prometheus metrics endpoint at /metrics
# enabled = true

# Metrics collection interval in seconds
# collection_interval = 30

# ======================
# Data Source (Optional - can be configured via UI)
# ======================
[datasource]
# Type: influx, timescale, http_json
# type = "influx"

# InfluxDB 1.x Configuration
# [datasource.influx]
# host = "influxdb"
# port = 8086
# database = "historian_test"
# username = "historian"
# password = "historian123"

# ======================
# Advanced Settings
# ======================
[advanced]
# Enable experimental features
# experimental = false

# Arrow Flight server (high-performance streaming)
# arrow_flight_enabled = false
# arrow_flight_port = 8081
