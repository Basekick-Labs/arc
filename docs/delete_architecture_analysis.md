# DELETE Architecture Analysis: File-Level vs Row-Level

## The Question

Can we apply the retention policy approach (file-level deletion) to general DELETE operations?

Let's analyze the trade-offs.

---

## Current Approaches

### 1. Row-Level DELETE (Currently Disabled)
**How it works:**
- Compute SHA256 hash for every row on write
- Store tombstones in `.deletes/*.parquet` files
- Filter tombstones during queries

**Status:** âŒ Disabled due to 4.6x performance regression

### 2. File-Level DELETE (Retention Policies)
**How it works:**
- Analyze entire Parquet files
- Delete files where ALL rows match criteria
- Physical file deletion, no tombstones

**Status:** âœ… Enabled, zero performance impact

---

## Detailed Comparison

### Architecture 1: Row-Level DELETE with Tombstones (Current/Disabled)

```
WRITE PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Row arrives                         â”‚
â”‚  â†“                                  â”‚
â”‚ Compute SHA256(time + tags)  â† SLOWâ”‚ +500Î¼s per row
â”‚  â†“                                  â”‚
â”‚ Write to Parquet with hash          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DELETE PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DELETE WHERE host='server01'        â”‚
â”‚  â†“                                  â”‚
â”‚ Query matching rows (with hashes)   â”‚
â”‚  â†“                                  â”‚
â”‚ Write tombstone file:               â”‚
â”‚   .deletes/delete_20251023.parquet  â”‚
â”‚   [hash1, hash2, hash3...]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUERY PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT * FROM cpu                   â”‚
â”‚  â†“                                  â”‚
â”‚ Read data files                     â”‚
â”‚  â†“                                  â”‚
â”‚ Read tombstone files                â”‚
â”‚  â†“                                  â”‚
â”‚ Filter: WHERE hash NOT IN tombstonesâ”‚ â† SLOW on every query
â”‚  â†“                                  â”‚
â”‚ Return filtered results             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Impact:**
```
Write: 1.74ms â†’ 8ms p50 (4.6x slower)   âŒ
Query: Added tombstone filtering       âŒ
Storage: Extra tombstone files         âŒ
```

---

### Architecture 2: File-Level DELETE (Like Retention)

```
WRITE PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Row arrives                         â”‚
â”‚  â†“                                  â”‚
â”‚ Write to Parquet (no hash)    â† FASTâ”‚ Normal speed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DELETE PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DELETE WHERE time < '2025-07-18'    â”‚
â”‚  â†“                                  â”‚
â”‚ Find files with ALL rows matching   â”‚
â”‚  â†“                                  â”‚
â”‚ Delete entire files                 â”‚
â”‚   os.unlink(file.parquet)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUERY PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT * FROM cpu                   â”‚
â”‚  â†“                                  â”‚
â”‚ Read data files (no filtering!)     â”‚
â”‚  â†“                                  â”‚
â”‚ Return results                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Impact:**
```
Write: 1.74ms (unchanged)              âœ…
Query: No overhead (unchanged)         âœ…
Storage: No extra files                âœ…
```

---

## Architecture 3: HYBRID - Rewrite-Based DELETE (New Proposal)

Instead of tombstones, **rewrite files** without deleted rows.

```
WRITE PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Row arrives                         â”‚
â”‚  â†“                                  â”‚
â”‚ Write to Parquet (no hash)    â† FASTâ”‚ Normal speed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DELETE PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DELETE WHERE host='server01'        â”‚
â”‚  â†“                                  â”‚
â”‚ 1. Find files containing matches    â”‚
â”‚    (use Parquet metadata filters)   â”‚
â”‚  â†“                                  â”‚
â”‚ 2. For each affected file:          â”‚
â”‚    - Read into Arrow table          â”‚
â”‚    - Filter: WHERE NOT (condition)  â”‚
â”‚    - Write new file (filtered)      â”‚
â”‚    - Delete old file                â”‚
â”‚  â†“                                  â”‚
â”‚ 3. Update query cache               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

QUERY PATH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SELECT * FROM cpu                   â”‚
â”‚  â†“                                  â”‚
â”‚ Read data files (no filtering!)     â”‚
â”‚  â†“                                  â”‚
â”‚ Return results                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance Impact:**
```
Write: 1.74ms (unchanged)              âœ…
Query: No overhead (unchanged)         âœ…
Delete: Expensive (rewrites) but rare  âš ï¸
Storage: No extra files                âœ…
```

---

## Pros & Cons Analysis

### Option 1: Row-Level DELETE with Tombstones (Current)

**Pros:**
- âœ… Precise row-level deletion
- âœ… Fast delete operation (just write tombstone)
- âœ… Can delete specific rows by any criteria

**Cons:**
- âŒ **CRITICAL:** 4.6x write slowdown (hash computation)
- âŒ **CRITICAL:** Query slowdown (tombstone filtering)
- âŒ Extra storage for tombstone files
- âŒ Requires compaction to physically remove deleted data
- âŒ Breaks zero-copy architecture

**Use Cases:**
- Delete specific error events
- GDPR compliance (delete user data)
- Remove corrupted data points

**Verdict:** âŒ **Performance cost too high for time-series workload**

---

### Option 2: File-Level DELETE (Retention-Style)

**Pros:**
- âœ… **ZERO** write overhead
- âœ… **ZERO** query overhead
- âœ… No tombstone files
- âœ… Simple implementation
- âœ… Physical deletion (no lingering data)

**Cons:**
- âŒ **CRITICAL:** Can only delete if ALL rows in file match
- âŒ Not suitable for sparse deletions
- âŒ Requires time-aligned data (compaction helps)

**Use Cases:**
- âœ… Retention policies (time-based)
- âœ… Delete entire partitions (time ranges)
- âŒ Can't delete specific hosts/tags across time
- âŒ Can't delete individual error events

**Example Limitations:**
```sql
-- âœ… WORKS: Delete old data
DELETE WHERE time < '2025-07-18'
-- Entire files are old â†’ delete them

-- âŒ DOESN'T WORK: Delete specific host
DELETE WHERE host = 'server01'
-- Host data is mixed across files â†’ can't delete files

-- âŒ DOESN'T WORK: Delete error events
DELETE WHERE error_code IS NOT NULL
-- Errors scattered across files â†’ can't delete files
```

**Verdict:** âœ… **Perfect for retention, LIMITED for general DELETE**

---

### Option 3: HYBRID - Rewrite-Based DELETE (Proposed)

**Pros:**
- âœ… **ZERO** write overhead (no hashing)
- âœ… **ZERO** query overhead (no tombstones)
- âœ… Precise row-level deletion
- âœ… Can delete by any criteria
- âœ… Physical deletion (no lingering data)
- âœ… No tombstone files

**Cons:**
- âš ï¸ Delete operation is EXPENSIVE (rewrites affected files)
- âš ï¸ Temporary increased storage (old + new files)
- âš ï¸ Locks required during rewrite (prevents concurrent writes)
- âš ï¸ Not suitable for frequent deletes

**Use Cases:**
- âœ… Infrequent, precise deletions
- âœ… GDPR compliance (delete user data)
- âœ… Remove corrupted data
- âŒ Not for high-frequency deletes

**Performance Characteristics:**
```
Write: No impact                       âœ…
Query: No impact                       âœ…
Delete: Slow but acceptable for rare ops âš ï¸

Example DELETE performance:
- 10 files affected, 100MB each
- Read: 1000MB @ 500MB/s = 2s
- Filter: Arrow compute = ~100ms
- Write: 900MB @ 300MB/s = 3s
- Total: ~5-6 seconds for 10 files

But happens RARELY (not on hot path)
```

**Verdict:** âœ… **Best hybrid approach for infrequent precise deletes**

---

## Recommendation: Tiered DELETE Strategy

Implement **BOTH** approaches based on use case:

### Tier 1: File-Level DELETE (Fast Path)
**For time-based deletions:**
```sql
-- Use file-level deletion (retention-style)
DELETE WHERE time < '2025-07-18'
DELETE WHERE time BETWEEN '2025-01-01' AND '2025-01-31'
```

**Implementation:** Already done (retention policies)

**Performance:**
- 1-2ms per file analysis
- Zero write/query impact
- Perfect for retention

---

### Tier 2: Rewrite-Based DELETE (Slow Path)
**For precise row-level deletions:**
```sql
-- Use rewrite-based deletion
DELETE WHERE host = 'server01' AND time > '2025-01-01'
DELETE WHERE error_code = 500
DELETE WHERE user_id = 'user123'  -- GDPR
```

**Implementation:** New (needs to be built)

**Performance:**
- Expensive (rewrites files)
- But happens rarely
- Zero impact on writes/queries

---

## Implementation Comparison

### Current (Disabled): Tombstone-Based
```python
# WRITE - adds overhead âŒ
def write_row(row):
    hash = sha256(row.time + row.tags)  # SLOW
    row['_hash'] = hash
    buffer.append(row)

# DELETE - fast âœ…
def delete(where):
    rows = query(where)
    hashes = [r['_hash'] for r in rows]
    write_tombstone_file(hashes)

# QUERY - adds overhead âŒ
def query(sql):
    data = read_parquet_files()
    tombstones = read_tombstone_files()  # SLOW
    return data.filter(~data._hash.isin(tombstones))
```

### Retention (Current): File-Level
```python
# WRITE - no overhead âœ…
def write_row(row):
    buffer.append(row)  # No hash

# DELETE - fast if file-aligned âœ…
def delete_old(cutoff_date):
    for file in parquet_files:
        max_time = read_max_time(file)  # Fast metadata
        if max_time < cutoff_date:
            os.unlink(file)  # Physical delete

# QUERY - no overhead âœ…
def query(sql):
    return read_parquet_files()  # No filtering
```

### Proposed: Rewrite-Based
```python
# WRITE - no overhead âœ…
def write_row(row):
    buffer.append(row)  # No hash

# DELETE - expensive but rare âš ï¸
def delete(where):
    affected_files = find_files_with_matches(where)

    for file in affected_files:
        # Rewrite file without deleted rows
        table = read_parquet(file)
        filtered = table.filter(~where)  # Arrow zero-copy

        new_file = write_parquet(filtered)
        os.unlink(file)
        os.rename(new_file, file)

# QUERY - no overhead âœ…
def query(sql):
    return read_parquet_files()  # No filtering
```

---

## Decision Matrix

| Deletion Type | Best Approach | Why |
|---------------|---------------|-----|
| **Time-based** (retention) | File-level | Files naturally aligned by time |
| **Partition drop** (entire day) | File-level | Can delete entire partition |
| **Tag-based** (specific host) | Rewrite-based | Rows scattered across files |
| **Error cleanup** | Rewrite-based | Errors scattered across files |
| **GDPR** (user data) | Rewrite-based | User data scattered across files |
| **Frequent deletes** | âŒ Not supported | Would kill performance |

---

## Recommended Implementation Plan

### Phase 1: Keep What Works (Now)
âœ… File-level deletion via retention policies
- Already implemented
- Zero performance impact
- Covers 80% of use cases (time-based cleanup)

### Phase 2: Add Rewrite-Based DELETE (Future)
ğŸ”¨ Implement rewrite-based deletion for precise cases:

```python
async def delete_with_rewrite(database, measurement, where_clause):
    """
    Delete rows by rewriting affected Parquet files.

    Use sparingly - this is expensive but preserves performance
    of normal write/query operations.
    """

    # 1. Find affected files using Parquet metadata
    affected_files = find_files_matching(where_clause)

    # 2. Rewrite each file
    for file in affected_files:
        table = pq.read_table(file)

        # Filter using Arrow compute (zero-copy)
        mask = evaluate_where_clause(table, where_clause)
        filtered = table.filter(~mask)

        # Write new file
        temp_file = write_temp_parquet(filtered)

        # Atomic replace
        os.replace(temp_file, file)

    # 3. Clear query cache for affected measurements
    query_cache.clear(database, measurement)
```

**Safety features:**
- Confirmation required (like current delete)
- Row count limits
- Dry-run support
- Execution time limits
- Lock during rewrite (prevent concurrent writes)

### Phase 3: Hybrid Router (Future)
Route DELETE to appropriate handler:

```python
def delete_router(sql):
    """Route DELETE to fastest implementation"""

    if is_time_based_only(sql):
        # Fast path: file-level deletion
        return delete_file_level(sql)
    else:
        # Slow path: rewrite-based deletion
        return delete_with_rewrite(sql)
```

---

## Performance Projections

### File-Level DELETE (Retention)
```
Files analyzed: 1000 files
Time per file: 1-2ms (metadata only)
Total time: 1-2 seconds
Write impact: ZERO
Query impact: ZERO
```

### Rewrite-Based DELETE
```
Files affected: 10 files (100MB each)
Read time: 2 seconds
Filter time: 100ms (Arrow compute)
Write time: 3 seconds
Total time: 5-6 seconds
Write impact: ZERO (not on write path)
Query impact: ZERO (no tombstones)

Acceptable for rare operations!
```

### Tombstone-Based DELETE (Don't Use)
```
Write impact: 4.6x slower    âŒ UNACCEPTABLE
Query impact: Significant    âŒ UNACCEPTABLE
Delete time: Fast           âœ… But not worth it
```

---

## Conclusion

**For Arc's time-series workload:**

1. âœ… **Use file-level deletion (retention-style)** for:
   - Time-based cleanup (90% of cases)
   - Retention policies
   - Partition drops

2. âœ… **Implement rewrite-based deletion** for:
   - GDPR compliance (rare)
   - Error cleanup (rare)
   - Tag-based deletion (rare)

3. âŒ **Never use tombstone-based deletion**:
   - Performance cost is unacceptable
   - Breaks zero-copy architecture
   - Slows down every write and query

**The key insight:** DELETE operations should be **rare** in time-series databases. When they happen, it's acceptable to spend 5-10 seconds rewriting files, because this doesn't impact the hot path (writes/queries).

---

## Next Steps

1. âœ… **Already done:** Retention policies (file-level deletion)
2. ğŸ”¨ **Implement next:** Rewrite-based DELETE for precise cases
3. ğŸ“Š **Monitor:** Track DELETE frequency to validate it's rare
4. ğŸš€ **Future:** Hybrid router for automatic best-path selection

Want me to implement the rewrite-based DELETE next?
